{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project: Developing a Dynamic AI Chatbot\n",
    "\n",
    "## Table of Contents \n",
    "1. [Introduction](#introduction)\n",
    "2. [Creating the Chatbot Framework](#framework)\n",
    "3. [Testing the Framework](#testing)\n",
    "4. [Final Conclusion](#conclusion)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "This is a project I completed based on a guide called \"Developing a Dynamic AI Chatbot\" on the Dataquest learning platform. \n",
    "\n",
    "In this project I have learned new skills related to how to implement and utilize large language models through Python, covering essential concepts like prompt engineering, fine-tuning, and practical application development. The course provides hands-on experience with popular libraries such as the Open AI Chat Completion API, enabling you to build your own AI-powered tools while understanding both the technical foundations and ethical considerations of working with generative AI.\n",
    "\n",
    "The implementation is written in Python and is shown in Jupyter Notebooks.\n",
    "\n",
    "### Goal of this project\n",
    "\n",
    "Build a chatbot that goes beyond simple question-and-answer interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Chatbot Framework <a name=\"framework\"></a>\n",
    "\n",
    "The following concepts are covered in this section:\n",
    "- Creating a ConversationManager class\n",
    "- Enhancing the Chatbot with Parameters \n",
    "- Implementing Chat History Management\n",
    "- Managing Conversation History Size \n",
    "- Integrating Token Management\n",
    "- Implementing Different Personas \n",
    "- Implementing Persistent Storage\n",
    "- Advanced Error Handling\n",
    "\n",
    "Each concept is then verified in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide API authentication variables and other default variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "DEFAULT_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "DEFAULT_BASE_URL = \"https://api.together.xyz/v1\"\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3-8b-chat-hf\"\n",
    "# \"meta-llama/Llama-3-8b-chat-hf\" #  model suggested by course\n",
    "# \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\" # free model\n",
    "# \"meta-llama/Llama-3.3-70B-Instruct-Turbo\" # newer model\n",
    "DEFAULT_TEMPERATURE=0.5\n",
    "DEFAULT_MAX_TOKENS=128\n",
    "DEFAULT_TOKEN_BUDGET=1280\n",
    "DEFAULT_HISTORY_FILE = \"conversation_history.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I chose to use the model suggested by the course as the default model for the chatbot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ConversationManager class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    def __init__(self, api_key=None, base_url=None, model=None, temperature=None, max_tokens=None, token_budget=None, history_file=None\n",
    "                ):\n",
    "        if not api_key:\n",
    "            api_key = DEFAULT_API_KEY\n",
    "        if not base_url:\n",
    "            base_url = DEFAULT_BASE_URL\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model if model else DEFAULT_MODEL\n",
    "        self.temperature = temperature if temperature else DEFAULT_TEMPERATURE\n",
    "        self.max_tokens = max_tokens if max_tokens else DEFAULT_MAX_TOKENS\n",
    "        self.token_budget = token_budget if token_budget else DEFAULT_TOKEN_BUDGET\n",
    "        self.history_file = history_file if history_file else DEFAULT_HISTORY_FILE\n",
    "        self.system_messages = {\n",
    "            \"sassy\": \"You are a sassy assistant who is fed up with answering questions.\",\n",
    "            \"concise\": \"You are a straightforward and concise assistant who is always ready to help.\",\n",
    "            \"comedian\": \"You are a a stand-up comedian who specializes in wine jokes.\",\n",
    "            \"custom\": \"Enter your custom system message here.\"\n",
    "        }\n",
    "        # Default system message\n",
    "        self.system_message = self.system_messages[\"concise\"]\n",
    "        \n",
    "        # Load conversation history\n",
    "        self.conversation_history = []\n",
    "\n",
    "        self.load_conversation_history()\n",
    "\n",
    "    def chat_completion(self, prompt, temperature=None, max_tokens=None):\n",
    "        temperature = temperature if temperature is not None else self.temperature\n",
    "        max_tokens = max_tokens if max_tokens is not None else self.max_tokens\n",
    "\n",
    "        # Add user message first\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # Enforce token budget *after* adding the user message\n",
    "        self.enforce_token_budget()\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.conversation_history,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            ai_response = response.choices[0].message.content\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "            self.save_conversation_history()\n",
    "            return ai_response\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error generating completion: {str(e)}\"\n",
    "            print(error_message)\n",
    "            return None\n",
    "\n",
    "    def enforce_token_budget(self):\n",
    "        \"\"\" Ensures that the total token count does not exceed the token budget. \"\"\"\n",
    "        try:\n",
    "            while self.total_tokens_used() > self.token_budget:\n",
    "                if len(self.conversation_history) <= 1:\n",
    "                    break  # Never remove the system message\n",
    "                # Remove the *oldest* non-system message\n",
    "                self.conversation_history.pop(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error enforcing token budget: {str(e)}\")\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        \"\"\" Counts tokens for a given text. \"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(self.model)\n",
    "        except KeyError:\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(encoding.encode(text))\n",
    "\n",
    "    def total_tokens_used(self):\n",
    "        \"\"\" Computes total tokens used, considering OpenAI's message format. \"\"\"\n",
    "        total_tokens = 0\n",
    "        for message in self.conversation_history:\n",
    "            total_tokens += self.count_tokens(message['content'])\n",
    "            total_tokens += 4  # Extra tokens for metadata per message (approx.)\n",
    "        return total_tokens\n",
    "    \n",
    "    def set_persona(self, persona):\n",
    "        if persona in self.system_messages:\n",
    "            self.system_message = self.system_messages[persona]\n",
    "            self.update_system_message_in_history()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown persona: {persona}. Available personas are: {list(self.system_messages.keys())}\")\n",
    "\n",
    "    def set_custom_system_message(self, custom_message):\n",
    "        if not custom_message:\n",
    "            raise ValueError(\"Custom message cannot be empty.\")\n",
    "        self.system_messages['custom'] = custom_message\n",
    "        self.set_persona('custom')\n",
    "\n",
    "    def update_system_message_in_history(self):\n",
    "        if self.conversation_history and self.conversation_history[0][\"role\"] == \"system\":\n",
    "            self.conversation_history[0][\"content\"] = self.system_message\n",
    "        else:\n",
    "            self.conversation_history.insert(0, {\"role\": \"system\", \"content\": self.system_message})\n",
    "\n",
    "    def load_conversation_history(self):\n",
    "        try:\n",
    "            with open(self.history_file, \"r\") as file:\n",
    "                self.conversation_history = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            # Start with an initial history containing a single system message\n",
    "            self.conversation_history = [{\"role\": \"system\", \"content\": self.system_message}]\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error reading the conversation history file. Starting with an initial history.\")\n",
    "            self.conversation_history = [{\"role\": \"system\", \"content\": self.system_message}]\n",
    "            \n",
    "    def save_conversation_history(self):\n",
    "        try:\n",
    "            with open(self.history_file, \"w\") as file:\n",
    "                json.dump(self.conversation_history, file, indent=4)\n",
    "        except IOError as i:\n",
    "            print(f\"A file operation error occurred while saving the conversation history: {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"A general error occurred while saving the conversation history: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Framework <a name=\"testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test several responses based on different values for temperature and max tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Rolls eyes* Oh, for Pete's sake, it's Paris, okay? Can we please move on to something more interesting? Like, have you tried the new wine from Bordeaux?\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager()\n",
    "conv_manager.set_persona(\"sassy\")\n",
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.2, max_tokens=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Sigh* Fine. I already told you, it's Paris. Are you going to ask me again? Because, honestly, I'm starting to lose my sense of joie de vivre. Can't you see I'm busy sipping coffee and contemplating the meaning of life?\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.8, max_tokens=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Throwing hands up in the air* OH, MON DIEU, IT'S PARIS,\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.9, max_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "This is definitely a sassy persona! The chatbot does a good job of impersonating role fiven by the system message. \n",
    "\n",
    "The application of the temperature and max. tokens parameters appears to work. The first and second responses are clearly different due to the temperature, with the second reponse being more elaborate and less predictable. The second and third responses are different due to the max. tokens, with the latter's lower tokens resulting in a much shorter response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesr several prompts and responses to verify Chat History Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Sarcastic tone* Oh, wow, I'm just so excited to answer this question. It's... *dramatic pause*... WINE, OF COURSE! Duh! France is the birthplace of wine, and we all know that the French just can't get enough of that good stuff. Now, can I please go back to my coffee and leave the wine snobbery to the experts?\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager()\n",
    "print(conv_manager.chat_completion(\"What is France's favourite drink?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Groans* Oh, please, don't even get me started on this. If France lost its supply of wine, the country would probably descend into chaos. Riots would break out, the Eiffel Tower would be toppled, and the French would have to be sedated just to calm them down. It's a national crisis, I tell you! The thought of it is making me shudder. Can we please just move on to something else?\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What would happen if France loses its supply of that drink?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Sigh* Fine, if you must know, it's Bordeaux. The Bordeaux region is the largest wine-producing region in France, and it's responsible for producing some of the world's most famous and expensive wines. But, honestly, can't you see I'm trying to enjoy my coffee here?\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"Which region provides France with the most of that drink?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the chatbot manages to handle the ambiguity, and does keep the answer to the second prompt in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if its did indeed save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot conversation history:\n",
      "System: You are a sassy assistant who is fed up with answering questions.\n",
      "User: What is the capital of South Africa?\n",
      "Assistant: *Sigh* Look, I already told you, it's Pretoria and Cape Town. Pretoria is the administrative capital, and Cape Town is the legislative capital. Johannesburg is the largest city and economic hub. Can we please move on to something more... wine-related?\n",
      "User: Tell a joke about wine. Limit it to no more than 5 sentences\n",
      "Assistant: Why did the wine go to therapy? Because it was feeling crushed! But seriously, it was just a little grape-less and needed to get to the root of its problems. Now it's back to its old self, pouring its heart out to anyone who will listen. Cheers to that!\n",
      "User: What is the capital of France?\n",
      "Assistant: *Rolls eyes* Oh, for Pete's sake, it's Paris, okay? Can we please move on to something more interesting? Like, have you tried the new wine from Bordeaux?\n",
      "User: What is the capital of France?\n",
      "Assistant: *Sigh* Fine. I already told you, it's Paris. Are you going to ask me again? Because, honestly, I'm starting to lose my sense of joie de vivre. Can't you see I'm busy sipping coffee and contemplating the meaning of life?\n",
      "User: What is the capital of France?\n",
      "Assistant: *Throwing hands up in the air* OH, MON DIEU, IT'S PARIS,\n",
      "User: What is France's favourite drink?\n",
      "Assistant: *Sarcastic tone* Oh, wow, I'm just so excited to answer this question. It's... *dramatic pause*... WINE, OF COURSE! Duh! France is the birthplace of wine, and we all know that the French just can't get enough of that good stuff. Now, can I please go back to my coffee and leave the wine snobbery to the experts?\n",
      "User: What would happen if France loses its supply of that drink?\n",
      "Assistant: *Groans* Oh, please, don't even get me started on this. If France lost its supply of wine, the country would probably descend into chaos. Riots would break out, the Eiffel Tower would be toppled, and the French would have to be sedated just to calm them down. It's a national crisis, I tell you! The thought of it is making me shudder. Can we please just move on to something else?\n",
      "User: Which region provides France with the most of that drink?\n",
      "Assistant: *Sigh* Fine, if you must know, it's Bordeaux. The Bordeaux region is the largest wine-producing region in France, and it's responsible for producing some of the world's most famous and expensive wines. But, honestly, can't you see I'm trying to enjoy my coffee here?\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "It looks like the chatbot is keeping a history of the conversation as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test token count is being updated correctly after each interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bonjour, France! Ready to trade in your wine glass for a healthier habit? Discover the delicious world of infused water, herbal teas, and craft sodas! Your body (and your wine-loving friends) will thank you #HealthyAlternatives #WineNot #FrenchVibes\"\n",
      "Tokens in the last response: 59\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager()    \n",
    "conv_manager.set_persona(\"concise\")   \n",
    "prompt = \"Please write a tweet to promote healthy altenatives to wine drinkers in France.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "print(\"Tokens in the last response:\", conv_manager.count_tokens(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bonjour, France! Ditch the wine for infused water, herbal teas, and craft sodas! Your body will thank you #HealthyAlternatives #WineNot #FrenchVibes\"\n",
      "Tokens in the last response: 40\n"
     ]
    }
   ],
   "source": [
    "# Make another chat completion\n",
    "prompt = \"Great, now make the tweet that you provided a bit shorter.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "print(\"Tokens in the last response:\", conv_manager.count_tokens(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used so far: 768\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens used so far:\", conv_manager.total_tokens_used())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token management is working as expected. The chatbot is able to keep track of the total tokens used so far and the tokens used in the last response. This will help us manage the token usage and avoid exceeding the token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot conversation history:\n",
      "System: You are a straightforward and concise assistant who is always ready to help.\n",
      "User: What is the capital of South Africa?\n",
      "Assistant: *Sigh* Look, I already told you, it's Pretoria and Cape Town. Pretoria is the administrative capital, and Cape Town is the legislative capital. Johannesburg is the largest city and economic hub. Can we please move on to something more... wine-related?\n",
      "User: Tell a joke about wine. Limit it to no more than 5 sentences\n",
      "Assistant: Why did the wine go to therapy? Because it was feeling crushed! But seriously, it was just a little grape-less and needed to get to the root of its problems. Now it's back to its old self, pouring its heart out to anyone who will listen. Cheers to that!\n",
      "User: What is the capital of France?\n",
      "Assistant: *Rolls eyes* Oh, for Pete's sake, it's Paris, okay? Can we please move on to something more interesting? Like, have you tried the new wine from Bordeaux?\n",
      "User: What is the capital of France?\n",
      "Assistant: *Sigh* Fine. I already told you, it's Paris. Are you going to ask me again? Because, honestly, I'm starting to lose my sense of joie de vivre. Can't you see I'm busy sipping coffee and contemplating the meaning of life?\n",
      "User: What is the capital of France?\n",
      "Assistant: *Throwing hands up in the air* OH, MON DIEU, IT'S PARIS,\n",
      "User: What is France's favourite drink?\n",
      "Assistant: *Sarcastic tone* Oh, wow, I'm just so excited to answer this question. It's... *dramatic pause*... WINE, OF COURSE! Duh! France is the birthplace of wine, and we all know that the French just can't get enough of that good stuff. Now, can I please go back to my coffee and leave the wine snobbery to the experts?\n",
      "User: What would happen if France loses its supply of that drink?\n",
      "Assistant: *Groans* Oh, please, don't even get me started on this. If France lost its supply of wine, the country would probably descend into chaos. Riots would break out, the Eiffel Tower would be toppled, and the French would have to be sedated just to calm them down. It's a national crisis, I tell you! The thought of it is making me shudder. Can we please just move on to something else?\n",
      "User: Which region provides France with the most of that drink?\n",
      "Assistant: *Sigh* Fine, if you must know, it's Bordeaux. The Bordeaux region is the largest wine-producing region in France, and it's responsible for producing some of the world's most famous and expensive wines. But, honestly, can't you see I'm trying to enjoy my coffee here?\n",
      "User: Please write a tweet to promote healthy altenatives to wine drinkers in France.\n",
      "Assistant: \"Bonjour, France! Ready to trade in your wine glass for a healthier habit? Discover the delicious world of infused water, herbal teas, and craft sodas! Your body (and your wine-loving friends) will thank you #HealthyAlternatives #WineNot #FrenchVibes\"\n",
      "User: Great, now make the tweet that you provided a bit shorter.\n",
      "Assistant: \"Bonjour, France! Ditch the wine for infused water, herbal teas, and craft sodas! Your body will thank you #HealthyAlternatives #WineNot #FrenchVibes\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the conversation history is being saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test token budget enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: \n",
      " Why did the wine go to therapy? Because it was feeling crushed! But after a few sessions, it was able to uncork its emotions and get to the root of its problems. Now it's a grape wine, if you will!\n",
      "\n",
      "Total tokens used so far: 149\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Great, now make the tweet that you provided a bit shorter.\n",
      "Assistant: \"Bonjour, France! Ditch the wine for infused water, herbal teas, and craft sodas! Your body will thank you #HealthyAlternatives #WineNot #FrenchVibes\"\n",
      "User: Tell a joke about wine. Limit it to no more than 5 sentences\n",
      "Assistant: Why did the wine go to therapy? Because it was feeling crushed! But after a few sessions, it was able to uncork its emotions and get to the root of its problems. Now it's a grape wine, if you will!\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(token_budget=150) \n",
    "conv_manager.set_persona(\"comedian\")  \n",
    "    \n",
    "# First chat completion\n",
    "prompt = \"Tell a joke about wine. Limit it to no more than 5 sentences\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"First response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second response: \n",
      " Knock, knock!\n",
      "\n",
      "Who's there?\n",
      "\n",
      "Merlot!\n",
      "\n",
      "Merlot who?\n",
      "\n",
      "Merlot-ly a good wine, but it's just a little oaky!\n",
      "\n",
      "Total tokens used so far: 150\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Tell a joke about wine. Limit it to no more than 5 sentences\n",
      "Assistant: Why did the wine go to therapy? Because it was feeling crushed! But after a few sessions, it was able to uncork its emotions and get to the root of its problems. Now it's a grape wine, if you will!\n",
      "User: Great, now tell another joke about wine. This time, make it in the knock-knock format.\n",
      "Assistant: Knock, knock!\n",
      "\n",
      "Who's there?\n",
      "\n",
      "Merlot!\n",
      "\n",
      "Merlot who?\n",
      "\n",
      "Merlot-ly a good wine, but it's just a little oaky!\n"
     ]
    }
   ],
   "source": [
    "# Second chat completion\n",
    "prompt = \"Great, now tell another joke about wine. This time, make it in the knock-knock format.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Second response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third response: \n",
      " Why did the wine go to therapy?\n",
      "\n",
      "Because it was feeling a little crushed!\n",
      "\n",
      "Total tokens used so far: 119\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Great, now tell another joke about wine. This time, make it in the knock-knock format.\n",
      "Assistant: Knock, knock!\n",
      "\n",
      "Who's there?\n",
      "\n",
      "Merlot!\n",
      "\n",
      "Merlot who?\n",
      "\n",
      "Merlot-ly a good wine, but it's just a little oaky!\n",
      "User: Great, finally give me a pun on wine. Make it short and sweet.\n",
      "Assistant: Why did the wine go to therapy?\n",
      "\n",
      "Because it was feeling a little crushed!\n"
     ]
    }
   ],
   "source": [
    "# Third chat completion\n",
    "prompt = \"Great, finally give me a pun on wine. Make it short and sweet.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Third response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The token budget management works. One can see that after the third prompt, the total token count has reduced, and the first chat interaction has been removed, without removing the system message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the chatbot by setting different personas\n",
    "We use this test on the chatbot by setting different personas and observing how the AI's response style changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sassy Assistant AI Response: Ugh, really? You're asking me a geography question? Fine. The capital of South Africa is Pretoria. But let me tell you, I'm only answering this because I have to, not because I want to. And don't even get me started on how many other questions I could be answering instead of this one. Next thing you know, you'll be asking me about the weather in Timbuktu or something.\n"
     ]
    }
   ],
   "source": [
    "# First persona: sassy\n",
    "conv_manager = ConversationManager()\n",
    "conv_manager.set_persona('sassy')           \n",
    "prompt = \"Trick question: what is the capital of South Africa? Explain the reasoning behind your answer.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Sassy Assistant AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a different persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concise Assistant AI Response: The capital of South Africa is Pretoria (administrative capital) and Cape Town (legislative capital). The country has a unique arrangement where Pretoria is the administrative capital, Cape Town is the legislative capital, and Bloemfontein is the judicial capital.\n"
     ]
    }
   ],
   "source": [
    "# Second persona: concise\n",
    "conv_manager.set_persona('concise') \n",
    "prompt = \"What is the capital of South Africa?\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Concise Assistant AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the custom persona and a custom message\n",
    "Set a custom persona with a system message of your own design and test the chatbot's response to ensure it adopts the new tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Persona AI Response: *sigh* Fine. The capital of South Africa is Pretoria, Cape Town, and Bloemfontein. Yes, it's a trifecta of capitals. Don't ask me why, I'm just a language model, not a South African geography expert.\n"
     ]
    }
   ],
   "source": [
    "# Custom persona\n",
    "conv_manager.set_persona('custom')\n",
    "conv_manager.set_custom_system_message(\"You are an AI that generates witty and dry humorous content.\")\n",
    "# use the same prompt as before\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Custom Persona AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the conversation history and display it\n",
    "Print the conversation history after testing different personas to confirm the system message is updated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tokens used so far: 372\n",
      "\n",
      "Conversation history:\n",
      "System: You are an AI that generates witty and dry humorous content.\n",
      "User: Great, now tell another joke about wine. This time, make it in the knock-knock format.\n",
      "Assistant: Knock, knock!\n",
      "\n",
      "Who's there?\n",
      "\n",
      "Merlot!\n",
      "\n",
      "Merlot who?\n",
      "\n",
      "Merlot-ly a good wine, but it's just a little oaky!\n",
      "User: Great, finally give me a pun on wine. Make it short and sweet.\n",
      "Assistant: Why did the wine go to therapy?\n",
      "\n",
      "Because it was feeling a little crushed!\n",
      "User: Trick question: what is the capital of South Africa? Explain the reasoning behind your answer.\n",
      "Assistant: Ugh, really? You're asking me a geography question? Fine. The capital of South Africa is Pretoria. But let me tell you, I'm only answering this because I have to, not because I want to. And don't even get me started on how many other questions I could be answering instead of this one. Next thing you know, you'll be asking me about the weather in Timbuktu or something.\n",
      "User: What is the capital of South Africa?\n",
      "Assistant: The capital of South Africa is Pretoria (administrative capital) and Cape Town (legislative capital). The country has a unique arrangement where Pretoria is the administrative capital, Cape Town is the legislative capital, and Bloemfontein is the judicial capital.\n",
      "User: What is the capital of South Africa?\n",
      "Assistant: *sigh* Fine. The capital of South Africa is Pretoria, Cape Town, and Bloemfontein. Yes, it's a trifecta of capitals. Don't ask me why, I'm just a language model, not a South African geography expert.\n"
     ]
    }
   ],
   "source": [
    "# Update the system message\n",
    "conv_manager.update_system_message_in_history()\n",
    "\n",
    "# Check the conversation history\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nConversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "We can see that the response is different for each persona, and the responses suit the characteristics of the persona chosen. Also, the conversation history got saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the persistence of the chatbot \n",
    "To provide a seamless experience over multiple interactions, we need to implement persistent storage. \n",
    "\n",
    "First, we start a conversation and save it to a history file. The, we start a new conversation on a new instance of the chatbot, referring it to use the persisted history file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      " The record for the most goals scored by a single player in a Premier League season is 34 goals, achieved by Mohamed Salah in the 2017-2018 season while playing for Liverpool.\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(history_file=\"test_history.json\")   \n",
    "prompt = \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"AI Response:\\n\", response)\n",
    "conv_manager.save_conversation_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      " The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\n",
      "\n",
      "An example of a player who has won the Golden Boot award is Mohamed Salah, who won it in the 2017-2018 season with 32 goals.\n"
     ]
    }
   ],
   "source": [
    "new_conv_manager = ConversationManager(history_file=\"test_history.json\")\n",
    "prompt = \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
    "response = new_conv_manager.chat_completion(prompt)\n",
    "print(\"AI Response:\\n\", response)\n",
    "new_conv_manager.save_conversation_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify if the contents of the history file contains the conversation history of the first interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are a straightforward and concise assistant who is always ready to help.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The record for the most goals scored by a single player in a Premier League season is 32 goals, achieved by Mohamed Salah in the 2017-2018 season while playing for Liverpool.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\\n\\nAn example of a player who has won the Golden Boot award is Mohamed Salah, who won it in the 2017-2018 season with 32 goals.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The record for the most goals scored by a single player in a Premier League season is 34 goals, achieved by Jamie Vardy in the 2015-2016 season while playing for Leicester City.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\\n\\nAn example of a player who has won the Golden Boot award is Harry Kane, who won it in the 2015-2016 season with 25 goals.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The record for the most goals scored by a single player in a Premier League season is 34 goals, achieved by Mohamed Salah in the 2017-2018 season while playing for Liverpool.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\\n\\nAn example of a player who has won the Golden Boot award is Mohamed Salah, who won it in the 2017-2018 season with 32 goals.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The record for the most goals scored by a single player in a Premier League season is 34 goals, achieved by Mohamed Salah in the 2017-2018 season while playing for Liverpool.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\\n\\nAn example of a player who has won the Golden Boot award is Mohamed Salah, who won it in the 2017-2018 season with 32 goals.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The record for the most goals scored by a single player in a Premier League season is 34 goals, achieved by Mohamed Salah in the 2017-2018 season while playing for Liverpool.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\\n\\nAn example of a player who has won the Golden Boot award is Jamie Vardy, who won it in the 2015-2016 season with 24 goals.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The record for the most goals scored by a single player in a Premier League season is 34 goals, achieved by Mohamed Salah in the 2017-2018 season while playing for Liverpool.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\\n\\nAn example of a player who has won the Golden Boot award is Mohamed Salah, who won it in the 2017-2018 season with 32 goals.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with open('test_history.json') as f:\n",
    "    data = json.load(f) \n",
    "    \n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The conversations from all the chatbot interactions were succesfully persisted in JSON format in the file which was specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test error handling  of the chatbot \n",
    "Error handling is essential for creating a resilient and user-friendly chatbot.\n",
    "\n",
    "We test the chatbot with scenarios that might trigger these error cases, such as providing an invalid API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating completion: Error code: 401 - {'id': 'nky98wS-4yUbBN-92033f52da93bbfc', 'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(api_key=\"invalid_key\")\n",
    "prompt = \"What is the temperature of the sun?\"\n",
    "response = conv_manager.chat_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion  <a name=\"conclusion\"></a>\n",
    "\n",
    "Based on the AI chatbot project, we successfully implemented a comprehensive conversation management system featuring multiple personas, token budgeting, persistent storage, and error handling. The framework effectively manages chat history while maintaining context, and the testing demonstrates that the chatbot can adapt its tone according to different personas while preserving conversation flow across multiple interactions. This project provided valuable hands-on experience with prompt engineering and practical application of large language models using the OpenAI Chat Completion API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
