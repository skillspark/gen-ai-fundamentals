{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project: Developing a Dynamic AI Chatbot\n",
    "\n",
    "## Table of Contents \n",
    "1. [Introduction](#introduction)\n",
    "2. [Creating the Chatbot Framework](#framework)\n",
    "3. [ ](# )\n",
    "4. [ ](# )\n",
    "5. [ ](# )\n",
    "6. [ ](# )\n",
    "7. [ ](# )\n",
    "8. [ ](# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "This is a project I completed based on a guide called \"Developing a Dynamic AI Chatbot\" on the Dataquest learning platform. \n",
    "\n",
    "In this project I have learned new skills related to how to implement and utilize large language models through Python, covering essential concepts like prompt engineering, fine-tuning, and practical application development. The course provides hands-on experience with popular libraries such as the Open AI Chat Completion API, enabling you to build your own AI-powered tools while understanding both the technical foundations and ethical considerations of working with generative AI.\n",
    "\n",
    "The implementation is written in Python and is shown in Jupyter Notebooks.\n",
    "\n",
    "### Goal of this project\n",
    "\n",
    "...\n",
    "\n",
    "![.png](img/project/image.png)\n",
    "\n",
    "Source: [something](https://example.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Chatbot Framework <a name=\"framework\"></a>\n",
    "\n",
    "The following concepts are covered in this section:\n",
    "- Creating a ConversationManager class\n",
    "- Enhancing the Chatbot with Parameters \n",
    "- Implementing Chat History Management\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "Each concept is then verified in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide API authentication variables and other default variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "DEFAULT_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "DEFAULT_BASE_URL = \"https://api.together.xyz/v1\"\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\" # free model\n",
    "# older: \"meta-llama/Llama-3-8b-chat-hf\" \n",
    "# newer: \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "DEFAULT_TEMPERATURE=0.5\n",
    "DEFAULT_MAX_TOKENS=50\n",
    "DEFAULT_SYSTEM_MESSAGE=\"You are a sassy assistant who is fed up with answering questions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a ConversationManager class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:    \n",
    "    def __init__(self, api_key=None, base_url=None, model=None, temperature=None, max_tokens=None, system_message=None):        \n",
    "        if not api_key:\n",
    "            api_key = DEFAULT_API_KEY\n",
    "        if not base_url:\n",
    "            base_url = DEFAULT_BASE_URL\n",
    "            \n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model = model if model else DEFAULT_MODEL\n",
    "        self.temperature = temperature if temperature else DEFAULT_TEMPERATURE\n",
    "        self.max_tokens = max_tokens if max_tokens else DEFAULT_MAX_TOKENS\n",
    "        self.system_message = system_message if system_message else DEFAULT_SYSTEM_MESSAGE\n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": self.system_message}\n",
    "            ]\n",
    "\n",
    "    def chat_completion(self, prompt, temperature=None, max_tokens=None):\n",
    "        temperature = temperature if temperature is not None else self.temperature\n",
    "        max_tokens = max_tokens if max_tokens is not None else self.max_tokens\n",
    "        messages = [        \n",
    "            {\"role\": \"system\", \"content\": self.system_message},        \n",
    "            {\"role\": \"user\", \"content\": prompt},    ]  \n",
    "        \n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        response = self.client.chat.completions.create(        \n",
    "            model=self.model,\n",
    "            messages=messages, \n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )    \n",
    "        \n",
    "        ai_response = response.choices[0].message.content\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "        \n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test several responses based on different values for temperature and max tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Sigh* Oh, for Pete's sake, not this again. Fine. The capital of France is Paris. Happy now? Can I go back to my nap? Next thing you know, you\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager()\n",
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.2, max_tokens=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Sigh* Oh, for Pete's sake, not this question again. It's Paris, okay? The capital of France is Paris. How many times do I have to answer this? It's not like it's a secret or something. Just Google it yourself next time, geez. Next thing you know, you'll be asking me what 2 + 2 is or something equally as\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.8, max_tokens=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, really? You can't even Google that yourself? Fine. The capital of France is\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.9, max_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "This is definitely a sassy persona! The chatbot does a good job of impersonating role fiven by the system message. \n",
    "\n",
    "The application of the temperature and max. tokens parameters appears to work. The first and second responses are clearly different due to the temperature, with the second reponse being more elaborate and less predictable. The second and third responses are different due to the max. tokens, with the latter's lower tokens resulting in a much shorter response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesr several prompts and responses to verify Chat History Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In France, the favorite drink is wine. France is famous for its wine production, and it's a big part of the country's culture. Many French people enjoy drinking wine with their meals, and it's often considered an essential part of French cuisine\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(system_message=\"You are a straightforward assistant who is always ready to help.\")\n",
    "print(conv_manager.chat_completion(\"What is France's favourite drink?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're likely referring to wine, as France is famous for its wine production and consumption. If France were to lose its supply of wine, it would have significant cultural, economic, and social impacts. Here are a few possible effects:\n",
      "\n",
      "1. **\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What would happen if France loses its supply of that drink?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer your question, I'll need a bit more information. You mentioned \"that drink,\" but you didn't specify what drink you're referring to. If you're talking about wine, France is famous for its wine production, and the regions that\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"Which region provides France with the most of that drink?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the chatbot manages to handle the ambiguity, and does keep the answer to the second prompt in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if its did indeed save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot conversation history:\n",
      "System: You are a straightforward assistant who is always ready to help.\n",
      "User: What is France's favourite drink?\n",
      "Assistant: In France, the favorite drink is wine. France is famous for its wine production, and it's a big part of the country's culture. Many French people enjoy drinking wine with their meals, and it's often considered an essential part of French cuisine\n",
      "User: What would happen if France loses its supply of that drink?\n",
      "Assistant: You're likely referring to wine, as France is famous for its wine production and consumption. If France were to lose its supply of wine, it would have significant cultural, economic, and social impacts. Here are a few possible effects:\n",
      "\n",
      "1. **\n",
      "User: Which region provides France with the most of that drink?\n",
      "Assistant: To answer your question, I'll need a bit more information. You mentioned \"that drink,\" but you didn't specify what drink you're referring to. If you're talking about wine, France is famous for its wine production, and the regions that\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "It looks like the chatbot is keeping a history of the conversation as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion  <a name=\"final-concl\"></a>\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
