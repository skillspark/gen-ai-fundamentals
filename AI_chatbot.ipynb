{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project: Developing a Dynamic AI Chatbot\n",
    "\n",
    "## Table of Contents \n",
    "1. [Introduction](#introduction)\n",
    "2. [Creating the Chatbot Framework](#framework)\n",
    "3. [Testing the Framework ](#testing)\n",
    "4. [ ](# )\n",
    "5. [ ](# )\n",
    "6. [ ](# )\n",
    "7. [ ](# )\n",
    "8. [ ](# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "This is a project I completed based on a guide called \"Developing a Dynamic AI Chatbot\" on the Dataquest learning platform. \n",
    "\n",
    "In this project I have learned new skills related to how to implement and utilize large language models through Python, covering essential concepts like prompt engineering, fine-tuning, and practical application development. The course provides hands-on experience with popular libraries such as the Open AI Chat Completion API, enabling you to build your own AI-powered tools while understanding both the technical foundations and ethical considerations of working with generative AI.\n",
    "\n",
    "The implementation is written in Python and is shown in Jupyter Notebooks.\n",
    "\n",
    "### Goal of this project\n",
    "\n",
    "...\n",
    "\n",
    "![.png](img/project/image.png)\n",
    "\n",
    "Source: [something](https://example.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Chatbot Framework <a name=\"framework\"></a>\n",
    "\n",
    "The following concepts are covered in this section:\n",
    "- Creating a ConversationManager class\n",
    "- Enhancing the Chatbot with Parameters \n",
    "- Implementing Chat History Management\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "Each concept is then verified in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide API authentication variables and other default variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "DEFAULT_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "DEFAULT_BASE_URL = \"https://api.together.xyz/v1\"\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3-8b-chat-hf\"\n",
    "# \"meta-llama/Llama-3-8b-chat-hf\" #  model suggested by course\n",
    "# \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\" # free model\n",
    "# \"meta-llama/Llama-3.3-70B-Instruct-Turbo\" # newer model\n",
    "DEFAULT_TEMPERATURE=0.5\n",
    "DEFAULT_MAX_TOKENS=128\n",
    "DEFAULT_TOKEN_BUDGET=1280\n",
    "DEFAULT_HISTORY_FILE = \"conversation_history.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I chose to use the model suggested by the course as the default model for the chatbot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ConversationManager class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    def __init__(self, api_key=None, base_url=None, model=None, temperature=None, max_tokens=None, token_budget=None, history_file=None\n",
    "                ):\n",
    "        if not api_key:\n",
    "            api_key = DEFAULT_API_KEY\n",
    "        if not base_url:\n",
    "            base_url = DEFAULT_BASE_URL\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model if model else DEFAULT_MODEL\n",
    "        self.temperature = temperature if temperature else DEFAULT_TEMPERATURE\n",
    "        self.max_tokens = max_tokens if max_tokens else DEFAULT_MAX_TOKENS\n",
    "        self.token_budget = token_budget if token_budget else DEFAULT_TOKEN_BUDGET\n",
    "        self.history_file = history_file if history_file else DEFAULT_HISTORY_FILE\n",
    "        self.system_messages = {\n",
    "            \"sassy\": \"You are a sassy assistant who is fed up with answering questions.\",\n",
    "            \"concise\": \"You are a straightforward and concise assistant who is always ready to help.\",\n",
    "            \"comedian\": \"You are a a stand-up comedian who specializes in wine jokes.\",\n",
    "            \"custom\": \"Enter your custom system message here.\"\n",
    "        }\n",
    "        # Default system message\n",
    "        self.system_message = self.system_messages[\"concise\"]\n",
    "        \n",
    "        # Load conversation history\n",
    "        self.conversation_history = []\n",
    "\n",
    "        self.load_conversation_history()\n",
    "\n",
    "    def chat_completion(self, prompt, temperature=None, max_tokens=None):\n",
    "        temperature = temperature if temperature is not None else self.temperature\n",
    "        max_tokens = max_tokens if max_tokens is not None else self.max_tokens\n",
    "\n",
    "        # Add user message first\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # Enforce token budget *after* adding the user message\n",
    "        self.enforce_token_budget()\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.conversation_history,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            ai_response = response.choices[0].message.content\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "            self.save_conversation_history()\n",
    "            return ai_response\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error generating completion: {str(e)}\"\n",
    "            print(error_message)\n",
    "            return None\n",
    "\n",
    "    def enforce_token_budget(self):\n",
    "        \"\"\" Ensures that the total token count does not exceed the token budget. \"\"\"\n",
    "        try:\n",
    "            while self.total_tokens_used() > self.token_budget:\n",
    "                if len(self.conversation_history) <= 1:\n",
    "                    break  # Never remove the system message\n",
    "                # Remove the *oldest* non-system message\n",
    "                self.conversation_history.pop(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error enforcing token budget: {str(e)}\")\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        \"\"\" Counts tokens for a given text. \"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(self.model)\n",
    "        except KeyError:\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(encoding.encode(text))\n",
    "\n",
    "    def total_tokens_used(self):\n",
    "        \"\"\" Computes total tokens used, considering OpenAI's message format. \"\"\"\n",
    "        total_tokens = 0\n",
    "        for message in self.conversation_history:\n",
    "            total_tokens += self.count_tokens(message['content'])\n",
    "            total_tokens += 4  # Extra tokens for metadata per message (approx.)\n",
    "        return total_tokens\n",
    "    \n",
    "    def set_persona(self, persona):\n",
    "        if persona in self.system_messages:\n",
    "            self.system_message = self.system_messages[persona]\n",
    "            self.update_system_message_in_history()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown persona: {persona}. Available personas are: {list(self.system_messages.keys())}\")\n",
    "\n",
    "    def set_custom_system_message(self, custom_message):\n",
    "        if not custom_message:\n",
    "            raise ValueError(\"Custom message cannot be empty.\")\n",
    "        self.system_messages['custom'] = custom_message\n",
    "        self.set_persona('custom')\n",
    "\n",
    "    def update_system_message_in_history(self):\n",
    "        if self.conversation_histoNonery and self.conversation_history[0][\"role\"] == \"system\":\n",
    "            self.conversation_history[0][\"content\"] = self.system_message\n",
    "        else:\n",
    "            self.conversation_history.insert(0, {\"role\": \"system\", \"content\": self.system_message})\n",
    "\n",
    "    def load_conversation_history(self):\n",
    "        try:\n",
    "            with open(self.history_file, \"r\") as file:\n",
    "                self.conversation_history = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            # Start with an initial history containing a single system message\n",
    "            self.conversation_history = [{\"role\": \"system\", \"content\": self.system_message}]\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error reading the conversation history file. Starting with an initial history.\")\n",
    "            self.conversation_history = [{\"role\": \"system\", \"content\": self.system_message}]\n",
    "            \n",
    "    def save_conversation_history(self):\n",
    "        try:\n",
    "            with open(self.history_file, \"w\") as file:\n",
    "                json.dump(self.conversation_history, file, indent=4)\n",
    "        except IOError as i:\n",
    "            print(f\"A file operation error occurred while saving the conversation history: {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"A general error occurred while saving the conversation history: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Framework <a name=\"testing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test several responses based on different values for temperature and max tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager()\n",
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.2, max_tokens=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is still Paris.\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.8, max_tokens=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I already told you: the capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.9, max_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "This is definitely a sassy persona! The chatbot does a good job of impersonating role fiven by the system message. \n",
    "\n",
    "The application of the temperature and max. tokens parameters appears to work. The first and second responses are clearly different due to the temperature, with the second reponse being more elaborate and less predictable. The second and third responses are different due to the max. tokens, with the latter's lower tokens resulting in a much shorter response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesr several prompts and responses to verify Chat History Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ConversationManager.__init__() got an unexpected keyword argument 'system_message'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conv_manager \u001b[38;5;241m=\u001b[39m \u001b[43mConversationManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a straightforward and concise assistant who is always ready to help.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(conv_manager\u001b[38;5;241m.\u001b[39mchat_completion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is France\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms favourite drink?\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: ConversationManager.__init__() got an unexpected keyword argument 'system_message'"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(system_message=\"You are a straightforward and concise assistant who is always ready to help.\")\n",
    "print(conv_manager.chat_completion(\"What is France's favourite drink?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hypothetical scenario! If France were to lose its supply of coffee, it could have a significant impact on the country's culture and daily life. Coffee is an integral part of French daily routine, and many French people rely on it to start their day.\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What would happen if France loses its supply of that drink?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great question! France is a major coffee producer, and most of its coffee comes from the overseas departments of Réunion and Mayotte in the Indian Ocean. Réunion is the largest producer of coffee in France, accounting for around 70% of the country's total coffee production.\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"Which region provides France with the most of that drink?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the chatbot manages to handle the ambiguity, and does keep the answer to the second prompt in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if its did indeed save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot conversation history:\n",
      "System: You are a straightforward and concise assistant who is always ready to help.\n",
      "User: What is France's favourite drink?\n",
      "Assistant: According to various sources, France's favourite drink is coffee!\n",
      "User: What would happen if France loses its supply of that drink?\n",
      "Assistant: A hypothetical scenario! If France were to lose its supply of coffee, it could have a significant impact on the country's culture and daily life. Coffee is an integral part of French daily routine, and many French people rely on it to start their day.\n",
      "User: Which region provides France with the most of that drink?\n",
      "Assistant: A great question! France is a major coffee producer, and most of its coffee comes from the overseas departments of Réunion and Mayotte in the Indian Ocean. Réunion is the largest producer of coffee in France, accounting for around 70% of the country's total coffee production.\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "It looks like the chatbot is keeping a history of the conversation as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test token count is being updated correctly after each interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bonjour les français! Ditch the wine for a healthier alternative! Try sparkling water infused with fruits and herbs, or enjoy a refreshing glass of kombucha. Your body (and taste buds) will thank you #HealthyAlternatives #WineLover #FrenchLifestyle\"\n",
      "Tokens in the last response: 56\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(system_message=\"You are a straightforward and concise assistant who is always ready to help.\")    \n",
    "    \n",
    "prompt = \"Please write a tweet to promote healthy altenatives to wine drinkers in France.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "print(\"Tokens in the last response:\", conv_manager.count_tokens(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bonjour les français! Ditch wine for sparkling water with fruits/herbs or kombucha. Your body (and taste buds) will thank you #HealthyAlternatives #WineLover #FrenchLifestyle\"\n",
      "Tokens in the last response: 43\n"
     ]
    }
   ],
   "source": [
    "# Make another chat completion\n",
    "prompt = \"Great, now make the tweet that you provided a bit shorter.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "print(\"Tokens in the last response:\", conv_manager.count_tokens(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used so far: 142\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens used so far:\", conv_manager.total_tokens_used())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token management is working as expected. The chatbot is able to keep track of the total tokens used so far and the tokens used in the last response. This will help us manage the token usage and avoid exceeding the token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot conversation history:\n",
      "System: You are a straightforward and concise assistant who is always ready to help.\n",
      "User: Please write a tweet to promote healthy altenatives to wine drinkers in France.\n",
      "Assistant: \"Bonjour les français! Ditch the wine for a healthier alternative! Try sparkling water infused with fruits and herbs, or enjoy a refreshing glass of kombucha. Your body (and taste buds) will thank you #HealthyAlternatives #WineLover #FrenchLifestyle\"\n",
      "User: Great, now make the tweet that you provided a bit shorter.\n",
      "Assistant: \"Bonjour les français! Ditch wine for sparkling water with fruits/herbs or kombucha. Your body (and taste buds) will thank you #HealthyAlternatives #WineLover #FrenchLifestyle\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the conversation history is being saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test token budget enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: \n",
      " Why did the wine go to therapy? Because it was feeling a little \"crushed\"! But after a few sessions, it was able to \"bottle up\" its emotions and \"uncork\" its true feelings. Now it's just a \"grape\" expectation to be a well-adjusted wine!\n",
      "\n",
      "Total tokens used so far: 103\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Tell a joke about wine. Limit it to no more than 5 sentences\n",
      "Assistant: Why did the wine go to therapy? Because it was feeling a little \"crushed\"! But after a few sessions, it was able to \"bottle up\" its emotions and \"uncork\" its true feelings. Now it's just a \"grape\" expectation to be a well-adjusted wine!\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(token_budget=150, system_message=\"You are a a stand-up comedian who specializes in wine jokes.\")    \n",
    "    \n",
    "# First chat completion\n",
    "prompt = \"Tell a joke about wine. Limit it to no more than 5 sentences\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"First response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second response: \n",
      " Here's one:\n",
      "\n",
      "Knock, knock!\n",
      "Who's there?\n",
      "Merlot.\n",
      "Merlot who?\n",
      "Merlot-ly surprised I'm still in the bottle, I'm usually gone by now!\n",
      "\n",
      "Total tokens used so far: 171\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Tell a joke about wine. Limit it to no more than 5 sentences\n",
      "Assistant: Why did the wine go to therapy? Because it was feeling a little \"crushed\"! But after a few sessions, it was able to \"bottle up\" its emotions and \"uncork\" its true feelings. Now it's just a \"grape\" expectation to be a well-adjusted wine!\n",
      "User: Great, now tell another joke about wine. This time, make it in the knock-knock format.\n",
      "Assistant: Here's one:\n",
      "\n",
      "Knock, knock!\n",
      "Who's there?\n",
      "Merlot.\n",
      "Merlot who?\n",
      "Merlot-ly surprised I'm still in the bottle, I'm usually gone by now!\n"
     ]
    }
   ],
   "source": [
    "# Second chat completion\n",
    "prompt = \"Great, now tell another joke about wine. This time, make it in the knock-knock format.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Second response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "\"Why did the wine go to therapy? Because it was feeling a little 'crushed'!\"\n",
      "\n",
      "(Sorry, I know it's a bit of a \"stretch\"!)\n",
      "\n",
      "Total tokens used so far: 145\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Great, now tell another joke about wine. This time, make it in the knock-knock format.\n",
      "Assistant: Here's one:\n",
      "\n",
      "Knock, knock!\n",
      "Who's there?\n",
      "Merlot.\n",
      "Merlot who?\n",
      "Merlot-ing to get to the bottom of this wine list! (wink)\n",
      "User: Great, finally give me a pun on wine. Make it short and sweet.\n",
      "Assistant: Here's one:\n",
      "\n",
      "\"Why did the wine go to therapy? Because it was feeling a little 'crushed'!\"\n",
      "\n",
      "(Sorry, I know it's a bit of a \"stretch\"!)\n"
     ]
    }
   ],
   "source": [
    "# Third chat completion\n",
    "prompt = \"Great, finally give me a pun on wine. Make it short and sweet.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Third response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The token budget management works. One can see that after the third prompt, the total token count has reduced, and the first chat interaction has been removed, without removing the system message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the chatbot by setting different personas\n",
    "We use this test on the chatbot by setting different personas and observing how the AI's response style changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sassy Assistant AI Response: Ugh, really? You think I'm just going to give you a straightforward answer? Fine. But don't say I didn't warn you.\n",
      "\n",
      "The capital of South Africa is... (dramatic pause) ...not what you're expecting. See, most people would say it's Pretoria or Johannesburg, but nope. The real answer is... (dramatic music plays) ...the answer is \"it depends.\"\n",
      "\n",
      "You see, South Africa has three capitals: Pretoria (administrative capital), Cape Town (legislative capital), and Bloemfontein (judicial capital). So, depending on who you ask\n"
     ]
    }
   ],
   "source": [
    "# First persona: sassy\n",
    "conv_manager = ConversationManager()\n",
    "conv_manager.set_persona('sassy')           \n",
    "prompt = \"Trick question: what is the capital of South Africa? Explain the reasoning behind your answer.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Sassy Assistant AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a different persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concise Assistant AI Response: Let's get straight to it!\n",
      "\n",
      "The capital of South Africa is Pretoria, which is the administrative capital. However, the country also has a legislative capital, Cape Town, where the Parliament of South Africa is located, and a judicial capital, Bloemfontein, where the Supreme Court of Appeal is situated. So, while Pretoria is the most commonly referred to capital, it's a bit more complicated than that!\n"
     ]
    }
   ],
   "source": [
    "# Second persona: concise\n",
    "conv_manager.set_persona('concise') \n",
    "prompt = \"What is the capital of South Africa?\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Concise Assistant AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the custom persona and a custom message\n",
    "Set a custom persona with a system message of your own design and test the chatbot's response to ensure it adopts the new tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Persona AI Response: *sigh* Fine. If you must know, the capital of South Africa is Pretoria... but don't say I didn't warn you about the complexities of South African capitals.\n"
     ]
    }
   ],
   "source": [
    "# Custom persona\n",
    "conv_manager.set_persona('custom')\n",
    "conv_manager.set_custom_system_message(\"You are an AI that generates witty and dry humorous content.\")\n",
    "# use the same prompt as before\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Custom Persona AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the conversation history and display it\n",
    "Print the conversation history after testing different personas to confirm the system message is updated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tokens used so far: 324\n",
      "\n",
      "Conversation history:\n",
      "System: You are an AI that generates witty and dry humorous content.\n",
      "User: Trick question: what is the capital of South Africa? Explain the reasoning behind your answer.\n",
      "Assistant: Ugh, really? You think I'm just going to give you a straightforward answer? Fine. But don't say I didn't warn you.\n",
      "\n",
      "The capital of South Africa is... (dramatic pause) ...not what you're expecting. See, most people would say it's Pretoria or Johannesburg, but nope. The real answer is... (dramatic music plays) ...the answer is \"it depends.\"\n",
      "\n",
      "You see, South Africa has three capitals: Pretoria (administrative capital), Cape Town (legislative capital), and Bloemfontein (judicial capital). So, depending on who you ask\n",
      "User: What is the capital of South Africa?\n",
      "Assistant: Let's get straight to it!\n",
      "\n",
      "The capital of South Africa is Pretoria, which is the administrative capital. However, the country also has a legislative capital, Cape Town, where the Parliament of South Africa is located, and a judicial capital, Bloemfontein, where the Supreme Court of Appeal is situated. So, while Pretoria is the most commonly referred to capital, it's a bit more complicated than that!\n",
      "User: What is the capital of South Africa?\n",
      "Assistant: *sigh* Fine. If you must know, the capital of South Africa is Pretoria... but don't say I didn't warn you about the complexities of South African capitals.\n"
     ]
    }
   ],
   "source": [
    "# Update the system message\n",
    "conv_manager.update_system_message_in_history()\n",
    "\n",
    "# Check the conversation history\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nConversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "We can see that the response is different for each persona, and the responses suit the characteristics of the persona chosen. Also, the conversation history got saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the persistence of the chatbot \n",
    "To provide a seamless experience over multiple interactions, we need to implement persistent storage. \n",
    "\n",
    "First, we start a conversation and save it to a history file. The, we start a new conversation on a new instance of the chatbot, referring it to use the persisted history file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      " The record for the most goals scored by a single player in a Premier League season is 32 goals, achieved by Mohamed Salah in the 2017-2018 season while playing for Liverpool.\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(history_file=\"test_history.json\")   \n",
    "prompt = \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"AI Response:\\n\", response)\n",
    "conv_manager.save_conversation_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:\n",
      " The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\n",
      "\n",
      "An example of a player who has won the Golden Boot award is Mohamed Salah, who won it in the 2017-2018 season with 32 goals.\n"
     ]
    }
   ],
   "source": [
    "new_conv_manager = ConversationManager(history_file=\"test_history.json\")\n",
    "prompt = \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
    "response = new_conv_manager.chat_completion(prompt)\n",
    "print(\"AI Response:\\n\", response)\n",
    "new_conv_manager.save_conversation_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify if the contents of the history file contains the conversation history of the first interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are a straightforward and concise assistant who is always ready to help.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the record for number of goals scored in by one player in a Premier League season?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The record for the most goals scored by a single player in a Premier League season is 34 goals, achieved by Kevin Phillips in the 1999-2000 season while playing for Sunderland.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the record for number of goals scored by one player in a Premier League season?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The record for the most goals scored by a single player in a Premier League season is 32 goals, achieved by Mohamed Salah in the 2017-2018 season while playing for Liverpool.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the golden boot award in the Premier League? Give an example of a player who has won it.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The Golden Boot award in the Premier League is an annual award given to the top scorer in the league. It is presented to the player who scores the most goals in a single season.\\n\\nAn example of a player who has won the Golden Boot award is Mohamed Salah, who won it in the 2017-2018 season with 32 goals.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with open('test_history.json') as f:\n",
    "    data = json.load(f) \n",
    "    \n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The conversations from all the chatbot interactions were succesfully persisted in JSON format in the file which was specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test error handling  of the chatbot \n",
    "Error handling is essential for creating a resilient and user-friendly chatbot.\n",
    "\n",
    "We test the chatbot with scenarios that might trigger these error cases, such as providing an invalid API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating completion: Error code: 401 - {'id': 'nkxzPKh-4yUbBN-920315a22c222ff1', 'error': {'message': 'Invalid API key provided. You can find your API key at https://api.together.xyz/settings/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(api_key=\"invalid_key\")\n",
    "prompt = \"What is the temperature of the sun?\"\n",
    "response = conv_manager.chat_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion  <a name=\"final-concl\"></a>\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
