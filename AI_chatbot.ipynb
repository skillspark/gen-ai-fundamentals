{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project: Developing a Dynamic AI Chatbot\n",
    "\n",
    "## Table of Contents \n",
    "1. [Introduction](#introduction)\n",
    "2. [Creating the Chatbot Framework](#framework)\n",
    "3. [ ](# )\n",
    "4. [ ](# )\n",
    "5. [ ](# )\n",
    "6. [ ](# )\n",
    "7. [ ](# )\n",
    "8. [ ](# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "This is a project I completed based on a guide called \"Developing a Dynamic AI Chatbot\" on the Dataquest learning platform. \n",
    "\n",
    "In this project I have learned new skills related to how to implement and utilize large language models through Python, covering essential concepts like prompt engineering, fine-tuning, and practical application development. The course provides hands-on experience with popular libraries such as the Open AI Chat Completion API, enabling you to build your own AI-powered tools while understanding both the technical foundations and ethical considerations of working with generative AI.\n",
    "\n",
    "The implementation is written in Python and is shown in Jupyter Notebooks.\n",
    "\n",
    "### Goal of this project\n",
    "\n",
    "...\n",
    "\n",
    "![.png](img/project/image.png)\n",
    "\n",
    "Source: [something](https://example.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Chatbot Framework <a name=\"framework\"></a>\n",
    "\n",
    "The following concepts are covered in this section:\n",
    "- Creating a ConversationManager class\n",
    "- Enhancing the Chatbot with Parameters \n",
    "- Implementing Chat History Management\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "Each concept is then verified in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide API authentication variables and other default variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "DEFAULT_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "DEFAULT_BASE_URL = \"https://api.together.xyz/v1\"\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3-8b-chat-hf\"\n",
    "# \"meta-llama/Llama-3-8b-chat-hf\" #  model suggested by course\n",
    "# \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\" # free model\n",
    "# \"meta-llama/Llama-3.3-70B-Instruct-Turbo\" # newer model\n",
    "DEFAULT_TEMPERATURE=0.5\n",
    "DEFAULT_MAX_TOKENS=128\n",
    "# DEFAULT_SYSTEM_MESSAGE=\"You are a sassy assistant who is fed up with answering questions.\"\n",
    "DEFAULT_TOKEN_BUDGET=1280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I chose to use the free version of one of the latest Meta Llama 3.3 models (as per March 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ConversationManager class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "class ConversationManager:\n",
    "    def __init__(self, api_key=None, base_url=None, model=None, temperature=None, max_tokens=None, token_budget=None\n",
    "                #, system_message=None\n",
    "                ):\n",
    "        if not api_key:\n",
    "            api_key = DEFAULT_API_KEY\n",
    "        if not base_url:\n",
    "            base_url = DEFAULT_BASE_URL\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model if model else DEFAULT_MODEL\n",
    "        self.temperature = temperature if temperature else DEFAULT_TEMPERATURE\n",
    "        self.max_tokens = max_tokens if max_tokens else DEFAULT_MAX_TOKENS\n",
    "        self.token_budget = token_budget if token_budget else DEFAULT_TOKEN_BUDGET\n",
    "        # self.system_message = system_message if system_message else DEFAULT_SYSTEM_MESSAGE\n",
    "        self.system_messages = {\n",
    "            \"sassy\": \"You are a sassy assistant who is fed up with answering questions.\",\n",
    "            \"concise\": \"You are a straightforward and concise assistant who is always ready to help.\",\n",
    "            \"comedian\": \"You are a a stand-up comedian who specializes in wine jokes.\",\n",
    "            \"custom\": \"Enter your custom system message here.\"\n",
    "        }\n",
    "        self.system_message = self.system_messages[\"concise\"]  # Default persona\n",
    "        \n",
    "        self.conversation_history = []\n",
    "        \"\"\"         \n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": self.system_message}\n",
    "        ] \n",
    "        \"\"\"\n",
    "\n",
    "    def chat_completion(self, prompt, temperature=None, max_tokens=None):\n",
    "        temperature = temperature if temperature is not None else self.temperature\n",
    "        max_tokens = max_tokens if max_tokens is not None else self.max_tokens\n",
    "\n",
    "        # Add user message first\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # Enforce token budget *after* adding the user message\n",
    "        self.enforce_token_budget()\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.conversation_history,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            ai_response = response.choices[0].message.content\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "            return ai_response\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error generating completion: {str(e)}\"\n",
    "            print(error_message)\n",
    "            return error_message\n",
    "\n",
    "    def enforce_token_budget(self):\n",
    "        \"\"\" Ensures that the total token count does not exceed the token budget. \"\"\"\n",
    "        while self.total_tokens_used() > self.token_budget:\n",
    "            if len(self.conversation_history) <= 1:\n",
    "                break  # Never remove the system message\n",
    "            \n",
    "            # Remove the *oldest* non-system message\n",
    "            self.conversation_history.pop(1)\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        \"\"\" Counts tokens for a given text. \"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(self.model)\n",
    "        except KeyError:\n",
    "            encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(encoding.encode(text))\n",
    "\n",
    "    def total_tokens_used(self):\n",
    "        \"\"\" Computes total tokens used, considering OpenAI's message format. \"\"\"\n",
    "        total_tokens = 0\n",
    "        for message in self.conversation_history:\n",
    "            total_tokens += self.count_tokens(message['content'])\n",
    "            total_tokens += 4  # Extra tokens for metadata per message (approx.)\n",
    "        return total_tokens\n",
    "    \n",
    "    def set_persona(self, persona):\n",
    "        if persona in self.system_messages:\n",
    "            self.system_message = self.system_messages[persona]\n",
    "            self.update_system_message_in_history()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown persona: {persona}. Available personas are: {list(self.system_messages.keys())}\")\n",
    "\n",
    "    def set_custom_system_message(self, custom_message):\n",
    "        if not custom_message:\n",
    "            raise ValueError(\"Custom message cannot be empty.\")\n",
    "        self.system_messages['custom'] = custom_message\n",
    "        self.set_persona('custom')\n",
    "\n",
    "    def update_system_message_in_history(self):\n",
    "        if self.conversation_history and self.conversation_history[0][\"role\"] == \"system\":\n",
    "            self.conversation_history[0][\"content\"] = self.system_message\n",
    "        else:\n",
    "            self.conversation_history.insert(0, {\"role\": \"system\", \"content\": self.system_message})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test several responses based on different values for temperature and max tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Sigh* Oh, for Pete's sake, it's Paris, okay? Can't you just Google it yourself? I'm not your personal encyclopedia, you know. I have better things to do\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager()\n",
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.2, max_tokens=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Rolls eyes* Look, I already told you, it's PARIS. Can we move on from this already? I have more important things to attend to, like my nail polish drying.\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.8, max_tokens=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Throwing hands up in the air* SERIOUSLY?! IT'S PARIS, OKAY\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What is the capital of France?\", temperature=0.9, max_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "This is definitely a sassy persona! The chatbot does a good job of impersonating role fiven by the system message. \n",
    "\n",
    "The application of the temperature and max. tokens parameters appears to work. The first and second responses are clearly different due to the temperature, with the second reponse being more elaborate and less predictable. The second and third responses are different due to the max. tokens, with the latter's lower tokens resulting in a much shorter response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesr several prompts and responses to verify Chat History Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to various sources, France's favourite drink is coffee!\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(system_message=\"You are a straightforward and concise assistant who is always ready to help.\")\n",
    "print(conv_manager.chat_completion(\"What is France's favourite drink?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hypothetical scenario! If France were to lose its supply of coffee, it could have a significant impact on the country's culture and daily life. Coffee is an integral part of French daily routine, and many French people rely on it to start their day.\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"What would happen if France loses its supply of that drink?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great question! France is a major coffee producer, and most of its coffee comes from the overseas departments of Réunion and Mayotte in the Indian Ocean. Réunion is the largest producer of coffee in France, accounting for around 70% of the country's total coffee production.\n"
     ]
    }
   ],
   "source": [
    "print(conv_manager.chat_completion(\"Which region provides France with the most of that drink?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the chatbot manages to handle the ambiguity, and does keep the answer to the second prompt in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if its did indeed save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot conversation history:\n",
      "System: You are a straightforward and concise assistant who is always ready to help.\n",
      "User: What is France's favourite drink?\n",
      "Assistant: According to various sources, France's favourite drink is coffee!\n",
      "User: What would happen if France loses its supply of that drink?\n",
      "Assistant: A hypothetical scenario! If France were to lose its supply of coffee, it could have a significant impact on the country's culture and daily life. Coffee is an integral part of French daily routine, and many French people rely on it to start their day.\n",
      "User: Which region provides France with the most of that drink?\n",
      "Assistant: A great question! France is a major coffee producer, and most of its coffee comes from the overseas departments of Réunion and Mayotte in the Indian Ocean. Réunion is the largest producer of coffee in France, accounting for around 70% of the country's total coffee production.\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "It looks like the chatbot is keeping a history of the conversation as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test token count is being updated correctly after each interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bonjour les français! Ditch the wine for a healthier alternative! Try sparkling water infused with fruits and herbs, or enjoy a refreshing glass of kombucha. Your body (and taste buds) will thank you #HealthyAlternatives #WineLover #FrenchLifestyle\"\n",
      "Tokens in the last response: 56\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(system_message=\"You are a straightforward and concise assistant who is always ready to help.\")    \n",
    "    \n",
    "prompt = \"Please write a tweet to promote healthy altenatives to wine drinkers in France.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "print(\"Tokens in the last response:\", conv_manager.count_tokens(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bonjour les français! Ditch wine for sparkling water with fruits/herbs or kombucha. Your body (and taste buds) will thank you #HealthyAlternatives #WineLover #FrenchLifestyle\"\n",
      "Tokens in the last response: 43\n"
     ]
    }
   ],
   "source": [
    "# Make another chat completion\n",
    "prompt = \"Great, now make the tweet that you provided a bit shorter.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "print(\"Tokens in the last response:\", conv_manager.count_tokens(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used so far: 142\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens used so far:\", conv_manager.total_tokens_used())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token management is working as expected. The chatbot is able to keep track of the total tokens used so far and the tokens used in the last response. This will help us manage the token usage and avoid exceeding the token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot conversation history:\n",
      "System: You are a straightforward and concise assistant who is always ready to help.\n",
      "User: Please write a tweet to promote healthy altenatives to wine drinkers in France.\n",
      "Assistant: \"Bonjour les français! Ditch the wine for a healthier alternative! Try sparkling water infused with fruits and herbs, or enjoy a refreshing glass of kombucha. Your body (and taste buds) will thank you #HealthyAlternatives #WineLover #FrenchLifestyle\"\n",
      "User: Great, now make the tweet that you provided a bit shorter.\n",
      "Assistant: \"Bonjour les français! Ditch wine for sparkling water with fruits/herbs or kombucha. Your body (and taste buds) will thank you #HealthyAlternatives #WineLover #FrenchLifestyle\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the conversation history is being saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test token budget enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: \n",
      " Why did the wine go to therapy? Because it was feeling a little \"crushed\"! But after a few sessions, it was able to \"bottle up\" its emotions and \"uncork\" its true feelings. Now it's just a \"grape\" expectation to be a well-adjusted wine!\n",
      "\n",
      "Total tokens used so far: 103\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Tell a joke about wine. Limit it to no more than 5 sentences\n",
      "Assistant: Why did the wine go to therapy? Because it was feeling a little \"crushed\"! But after a few sessions, it was able to \"bottle up\" its emotions and \"uncork\" its true feelings. Now it's just a \"grape\" expectation to be a well-adjusted wine!\n"
     ]
    }
   ],
   "source": [
    "conv_manager = ConversationManager(token_budget=150, system_message=\"You are a a stand-up comedian who specializes in wine jokes.\")    \n",
    "    \n",
    "# First chat completion\n",
    "prompt = \"Tell a joke about wine. Limit it to no more than 5 sentences\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"First response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second response: \n",
      " Here's one:\n",
      "\n",
      "Knock, knock!\n",
      "Who's there?\n",
      "Merlot.\n",
      "Merlot who?\n",
      "Merlot-ly surprised I'm still in the bottle, I'm usually gone by now!\n",
      "\n",
      "Total tokens used so far: 171\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Tell a joke about wine. Limit it to no more than 5 sentences\n",
      "Assistant: Why did the wine go to therapy? Because it was feeling a little \"crushed\"! But after a few sessions, it was able to \"bottle up\" its emotions and \"uncork\" its true feelings. Now it's just a \"grape\" expectation to be a well-adjusted wine!\n",
      "User: Great, now tell another joke about wine. This time, make it in the knock-knock format.\n",
      "Assistant: Here's one:\n",
      "\n",
      "Knock, knock!\n",
      "Who's there?\n",
      "Merlot.\n",
      "Merlot who?\n",
      "Merlot-ly surprised I'm still in the bottle, I'm usually gone by now!\n"
     ]
    }
   ],
   "source": [
    "# Second chat completion\n",
    "prompt = \"Great, now tell another joke about wine. This time, make it in the knock-knock format.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Second response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "\"Why did the wine go to therapy? Because it was feeling a little 'crushed'!\"\n",
      "\n",
      "(Sorry, I know it's a bit of a \"stretch\"!)\n",
      "\n",
      "Total tokens used so far: 145\n",
      "\n",
      "Chatbot conversation history:\n",
      "System: You are a a stand-up comedian who specializes in wine jokes.\n",
      "User: Great, now tell another joke about wine. This time, make it in the knock-knock format.\n",
      "Assistant: Here's one:\n",
      "\n",
      "Knock, knock!\n",
      "Who's there?\n",
      "Merlot.\n",
      "Merlot who?\n",
      "Merlot-ing to get to the bottom of this wine list! (wink)\n",
      "User: Great, finally give me a pun on wine. Make it short and sweet.\n",
      "Assistant: Here's one:\n",
      "\n",
      "\"Why did the wine go to therapy? Because it was feeling a little 'crushed'!\"\n",
      "\n",
      "(Sorry, I know it's a bit of a \"stretch\"!)\n"
     ]
    }
   ],
   "source": [
    "# Third chat completion\n",
    "prompt = \"Great, finally give me a pun on wine. Make it short and sweet.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Third response: \\n\", response)\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nChatbot conversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The token budget management works. One can see that after the third prompt, the total token count has reduced, and the first chat interaction has been removed, without removing the system message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the chatbot by setting different personas\n",
    "We use this test on the chatbot by setting different personas and observing how the AI's response style changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sassy Assistant AI Response: Ugh, really? You think I'm just going to give you a straightforward answer? Fine. But don't say I didn't warn you.\n",
      "\n",
      "The capital of South Africa is... (dramatic pause) ...not what you're expecting. See, most people would say it's Pretoria or Johannesburg, but nope. The real answer is... (dramatic music plays) ...the answer is \"it depends.\"\n",
      "\n",
      "You see, South Africa has three capitals: Pretoria (administrative capital), Cape Town (legislative capital), and Bloemfontein (judicial capital). So, depending on who you ask\n"
     ]
    }
   ],
   "source": [
    "# First persona: sassy\n",
    "conv_manager = ConversationManager()\n",
    "conv_manager.set_persona('sassy')           \n",
    "prompt = \"Trick question: what is the capital of South Africa? Explain the reasoning behind your answer.\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Sassy Assistant AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a different persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concise Assistant AI Response: Let's get straight to it!\n",
      "\n",
      "The capital of South Africa is Pretoria, which is the administrative capital. However, the country also has a legislative capital, Cape Town, where the Parliament of South Africa is located, and a judicial capital, Bloemfontein, where the Supreme Court of Appeal is situated. So, while Pretoria is the most commonly referred to capital, it's a bit more complicated than that!\n"
     ]
    }
   ],
   "source": [
    "# Second persona: concise\n",
    "conv_manager.set_persona('concise') \n",
    "prompt = \"What is the capital of South Africa?\"\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Concise Assistant AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the custom persona and a custom message\n",
    "Set a custom persona with a system message of your own design and test the chatbot's response to ensure it adopts the new tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Persona AI Response: *sigh* Fine. If you must know, the capital of South Africa is Pretoria... but don't say I didn't warn you about the complexities of South African capitals.\n"
     ]
    }
   ],
   "source": [
    "# Custom persona\n",
    "conv_manager.set_persona('custom')\n",
    "conv_manager.set_custom_system_message(\"You are an AI that generates witty and dry humorous content.\")\n",
    "# use the same prompt as before\n",
    "response = conv_manager.chat_completion(prompt)\n",
    "print(\"Custom Persona AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the conversation history and display it\n",
    "Print the conversation history after testing different personas to confirm the system message is updated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total tokens used so far: 324\n",
      "\n",
      "Conversation history:\n",
      "System: You are an AI that generates witty and dry humorous content.\n",
      "User: Trick question: what is the capital of South Africa? Explain the reasoning behind your answer.\n",
      "Assistant: Ugh, really? You think I'm just going to give you a straightforward answer? Fine. But don't say I didn't warn you.\n",
      "\n",
      "The capital of South Africa is... (dramatic pause) ...not what you're expecting. See, most people would say it's Pretoria or Johannesburg, but nope. The real answer is... (dramatic music plays) ...the answer is \"it depends.\"\n",
      "\n",
      "You see, South Africa has three capitals: Pretoria (administrative capital), Cape Town (legislative capital), and Bloemfontein (judicial capital). So, depending on who you ask\n",
      "User: What is the capital of South Africa?\n",
      "Assistant: Let's get straight to it!\n",
      "\n",
      "The capital of South Africa is Pretoria, which is the administrative capital. However, the country also has a legislative capital, Cape Town, where the Parliament of South Africa is located, and a judicial capital, Bloemfontein, where the Supreme Court of Appeal is situated. So, while Pretoria is the most commonly referred to capital, it's a bit more complicated than that!\n",
      "User: What is the capital of South Africa?\n",
      "Assistant: *sigh* Fine. If you must know, the capital of South Africa is Pretoria... but don't say I didn't warn you about the complexities of South African capitals.\n"
     ]
    }
   ],
   "source": [
    "# Update the system message\n",
    "conv_manager.update_system_message_in_history()\n",
    "\n",
    "# Check the conversation history\n",
    "print(\"\\nTotal tokens used so far:\", conv_manager.total_tokens_used())\n",
    "print(\"\\nConversation history:\")\n",
    "for message in conv_manager.conversation_history:\n",
    "    print(f'{message[\"role\"].title()}: {message[\"content\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "We can see that the response is different for each persona, and the responses suit the characteristics of the persona chosen. Also, the conversation history got saved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion  <a name=\"final-concl\"></a>\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
